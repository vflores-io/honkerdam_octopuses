{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the clean (intermediate) data\n",
    "pr_df = pd.read_parquet(\"data/intermediate_data/pr_df.parquet\", engine=\"pyarrow\")\n",
    "# pr_df = pr_df.head(10000)\n",
    "\n",
    "# check the shape\n",
    "print(pr_df.shape)\n",
    "\n",
    "# check the column names\n",
    "print(pr_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the `issues` column\n",
    "pr_df[\"issue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if there are null values in this column\n",
    "print(pr_df[\"issue\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's preprocess the text and lowercase everything and remove the \"title\" prefix\n",
    "print(pr_df['issue'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the issue title (leave the username_0 comment out for now)\n",
    "pr_df['issue_title'] = pr_df['issue'].str.split(\"username_0: \").str[0].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while we're are it, extract the user comments too\n",
    "pr_df['issue_comments'] = pr_df['issue'].str.split(\"username_0: \").str[1].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove special characters and markdown, whitespace\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove markdown\n",
    "    text = re.sub(r'[#!\\[\\]<>\\-*_|]+', '', text)\n",
    "    # remove whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "pr_df['issue_title_clean'] = pr_df['issue_title'].apply(clean_text)\n",
    "pr_df['issue_comments_clean'] = pr_df['issue_comments'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pr_df['issue_title_clean'].head())\n",
    "pr_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned data to disk for future use and a happy world\n",
    "pr_df.to_parquet(\"data/intermediate_data/pr_df_clean_issues.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the fun stuff\n",
    "\n",
    "### 1. Vectorize the Text\n",
    "\n",
    "We'll convert the `issue_title_cleaned` column into numerical representations using **TF-IDF** (Term Frequency-Inverse Document Frequency). This method is ideal for capturing the importance of words in a document relative to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# convert cleaned issue titles into a list\n",
    "issue_titles = pr_df['issue_title_clean'].dropna().tolist()\n",
    "\n",
    "# initialize and fit the tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')    # remove common stop words\n",
    "tfidf_matrix = vectorizer.fit_transform(issue_titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output matrix is a sparse matrix where each row represents an issue title as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'TF-IDF matrix shape: {tfidf_matrix.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute text similarities. We use the TF-IDF matrix to calculate pairwise cosine similarity, which measures how similar each issue title is to the others.\n",
    "\n",
    "NOTE: I tried to run this code:\n",
    "```py\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# calculate cosine similarity between issue titles\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix, dense_output=False)\n",
    "```\n",
    "\n",
    "but the jupyter kernel crashed. Probably the data set is too large. Let's try querying one single issue and compare it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_idx = 0 \n",
    "query_vector = tfidf_matrix[query_idx]\n",
    "\n",
    "# compute similarity of the query issue with all others\n",
    "similarity_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "# get the top N most similar issues\n",
    "top_indices = similarity_scores.argsort()[::-1][:10]\n",
    "for i in top_indices:\n",
    "    print(f\"similarity: {similarity_scores[i]:.2f} | title: {issue_titles[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that works, which means we can work in chunks. But before we try that, let's still try something else. Let's try using **FAISS** (Facebook AI Similarity Search).\n",
    "\n",
    "Note: Well that failed too. I did this:\n",
    "\n",
    "```py\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# convert the tf-idf matrix to numpy array\n",
    "dense_matrix = tfidf_matrix.toarray()\n",
    "\n",
    "# build faiss index\n",
    "dimension = dense_matrix.shape[1]             # number of features\n",
    "index = faiss.IndexFlatL2(dimension)          # build the index\n",
    "index.add(dense_matrix.astype(np.float32))    # add vectors to the index\n",
    "```\n",
    "\n",
    "So I guess I will resort to using the previous method but in chunks.\n",
    "\n",
    "We will process the rows of the TF-IDF matrix in manageable chunks and compute similarities agains the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "\n",
    "def compute_chunked_similarity(tfidf_matrix, chunk_size=1000, output_dir = \"data/intermediate_data/similarity_chunks/\"):\n",
    "    \"\"\"\n",
    "    compute cosine similarity in chunks to prevent memory overload.\n",
    "\n",
    "    parameters:\n",
    "    - tfidf_matrix: sparse matrix (tf-idf representation)\n",
    "    - chunk_size: number of rows to process in each chunk\n",
    "\n",
    "    returns:\n",
    "    - sparse similarity matrix\n",
    "    \"\"\"\n",
    "\n",
    "    num_rows = tfidf_matrix.shape[0]\n",
    "    similarity_chunks = []\n",
    "\n",
    "    for start_idx in range(0, num_rows, chunk_size):\n",
    "        end_idx = min(start_idx + chunk_size, num_rows)\n",
    "        print(f'processing rows {start_idx} to {end_idx}... ')\n",
    "\n",
    "        # compute similarity for the current chunk\n",
    "        chunk = tfidf_matrix[start_idx:end_idx]\n",
    "        chunk_similarity = cosine_similarity(chunk, tfidf_matrix, dense_output = False)\n",
    "\n",
    "        assert chunk_similarity.shape[1] == tfidf_matrix.shape[0], \"Chunk similarity shape does not match tfidf matrix shape\"\n",
    "\n",
    "        # save the sparse similarity matrix to disk to avoid RAM overload\n",
    "        output_path = os.path.join(output_dir, f'similarity_chunk_{start_idx}_{end_idx}.npz')\n",
    "        sp.save_npz(output_path, chunk_similarity)\n",
    "\n",
    "    print(\"all chunks processed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above \"solution\" is still quite a lot for my computer. So I just ran it once and produced some chunks for the similarity. Below is how one would run it:\n",
    "\n",
    "```py\n",
    "# run the cosine similarity in chunks\n",
    "similarity_matrix = compute_chunked_similarity(tfidf_matrix, chunk_size=1000)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honkerdam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
